import os
import json
import time
import requests
import pytz
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime

# ğŸ”¸ å·®åˆ†ä¿å­˜ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DATA_PATH = "previous.json"

# ğŸ”¹ å‰å›ã®ç‰©ä»¶ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
def load_previous(): 
    if os.path.exists(DATA_PATH):
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ğŸ”¹ ä»Šå›å–å¾—ã—ãŸç‰©ä»¶ãƒªã‚¹ãƒˆã‚’ä¿å­˜
def save_current(data):
    with open(DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ğŸ”¸ LINEé€šçŸ¥è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
GROUP_ID = os.environ.get("LINE_GROUP_ID")

# ğŸ”¹ LINEã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
def send_line_message(message):
    url = 'https://api.line.me/v2/bot/message/push'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {CHANNEL_ACCESS_TOKEN}'
    }
    body = {
        'to': GROUP_ID,
        'messages': [{'type': 'text', 'text': message}]
    }
    response = requests.post(url, headers=headers, data=json.dumps(body))
    print(f'ğŸ“¤ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}')
    print(f'ğŸ“¨ ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}')

# ğŸ”¹ Seleniumç”¨ã®Chromeãƒ‰ãƒ©ã‚¤ãƒã‚’ä½œæˆ
def create_driver():
    options = Options()
    options.add_argument("--headless")  # ç”»é¢ã‚’è¡¨ç¤ºã—ãªã„
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)
    return driver

# ğŸ”¹ é–“å–ã‚ŠãŒ2DKä»¥ä¸Šã‹ã‚’åˆ¤å®š
def is_layout_ok(layout):
    match = re.match(r"(\d+)[DLK]+", layout)
    if match:
        return int(match.group(1)) >= 2
    return False

# ğŸ”¹ é¢ç©ãŒ60ã¡ä»¥ä¸Šã‹ã‚’åˆ¤å®š
def is_size_ok(size_str):
    try:
        return float(size_str.replace("ã¡", "").strip()) >= 60.0
    except:
        return False

# ğŸ”¹ éšæ•°ãŒ3éšä»¥ä¸Šã‹ã‚’åˆ¤å®š
def is_floor_ok(floor_str):
    match = re.search(r"(\d+)éš", floor_str)
    if match:
        return int(match.group(1)) >= 3
    return False

# ğŸ”¹ ã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒ«æ±æœéœã®ãƒªãƒãƒ™æ¸ˆã¿ç‰©ä»¶ã‚’æŠ½å‡º
def fetch_renovated_higashi_asaka():
    url = "https://www.ur-net.go.jp/chintai/kanto/saitama/result/?line_station=14400_1488&todofuken=saitama"
    driver = create_driver()
    driver.get(url)
    time.sleep(5)  # JavaScriptã§ã®èª­ã¿è¾¼ã¿å¾…ã¡

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    listings = []
    base_url = "https://www.ur-net.go.jp"
    cards = soup.select("div.section_inner > ul > li")  # ç‰©ä»¶ã‚«ãƒ¼ãƒ‰ä¸€è¦§

    for card in cards:
        name_tag = card.select_one("p.property_name")
        if not name_tag or "ã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒ«æ±æœéœ" not in name_tag.text:
            continue  # ä»–ã®ç‰©ä»¶ã¯ã‚¹ã‚­ãƒƒãƒ—

        detail_link = card.select_one("a")
        if not detail_link:
            continue

        url = base_url + detail_link.get("href")
        title = name_tag.text.strip()

        # å„æƒ…å ±ã‚’æŠ½å‡º
        layout = card.select_one("p.layout")
        size = card.select_one("p.size")
        floor = card.select_one("p.floor")
        remarks = card.select_one("p.comment")

        layout_text = layout.text.strip() if layout else ""
        size_text = size.text.strip() if size else ""
        floor_text = floor.text.strip() if floor else ""
        remarks_text = remarks.text.strip() if remarks else ""

        # æ¡ä»¶ã«åˆè‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not is_layout_ok(layout_text):
            continue
        if not is_size_ok(size_text):
            continue
        if not is_floor_ok(floor_text):
            continue
        if "ãƒªãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³" not in remarks_text:
            continue

        # æ¡ä»¶ã‚’æº€ãŸã™ç‰©ä»¶ã‚’è¿½åŠ 
        listings.append({
            "title": f"{title} {layout_text} {size_text} {floor_text}",
            "url": url
        })

    return listings

# ğŸ”¹ URå…¬å¼ãŠçŸ¥ã‚‰ã›ãƒšãƒ¼ã‚¸ã‹ã‚‰æ–°ç¯‰ç‰©ä»¶ã‚’å–å¾—ï¼‹æ±æœéœãƒªãƒãƒ™ç‰©ä»¶ã‚‚è¿½åŠ 
def fetch_ur_listings():
    url = "https://www.ur-net.go.jp/chintai/information/"
    driver = create_driver()
    driver.get(url)
    wait = WebDriverWait(driver, 10)

    try:
        lottery_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ï¼ƒæŠ½é¸')]")))
        lottery_button.click()
    except:
        print("æŠ½é¸ãƒœã‚¿ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯ã«å¤±æ•—")

    try:
        kanto_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ï¼ƒé–¢æ±')]")))
        kanto_button.click()
    except:
        print("é–¢æ±ãƒœã‚¿ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯ã«å¤±æ•—")

    time.sleep(3)
    html = driver.page_source
    driver.quit()
    soup = BeautifulSoup(html, "html.parser")
    base_url = "https://www.ur-net.go.jp"
    links = soup.select("a.item_link.fc_blue.item_cell")

    listings = []
    for link in links:
        text = link.get_text(strip=True)
        href = link.get("href")
        if "æ–°ç¯‰è³ƒè²¸ä½å®…" in text:
            listings.append({"title": text, "url": base_url + href})

    # ğŸ” æ±æœéœãƒªãƒãƒ™ç‰©ä»¶ã‚‚è¿½åŠ ã§ãƒã‚§ãƒƒã‚¯ï¼
    listings += fetch_renovated_higashi_asaka()

    return listings

# ğŸ”¹ å‰å›ã¨ã®å·®åˆ†ã‚’æ¤œå‡ºï¼ˆtitle + url ã®ãƒšã‚¢ã§æ¯”è¼ƒï¼‰
def detect_new_listings(current, previous):
    previous_set = {(item["title"], item["url"]) for item in previous}
    return [item for item in current if (item["title"], item["url"]) not in previous_set]

# ğŸ”¹ ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    # ç¾åœ¨æ™‚åˆ»ï¼ˆJSTï¼‰ã‚’å–å¾—
    jst = pytz.timezone('Asia/Tokyo')
    now = datetime.now(jst)
    print(f"ğŸ•’ ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œæ™‚åˆ»ï¼ˆJSTï¼‰: {now.strftime('%Y-%m-%d %H:%M:%S')}")

    current = fetch_ur_listings()
    previous = load_previous()

    # åˆå›å®Ÿè¡Œã‚„ previous.json ãŒç©ºã®ã¨ãã¯é€šçŸ¥ã›ãšä¿å­˜ã ã‘
    if not previous:
        print("ğŸ“‚ åˆå›å®Ÿè¡Œã¾ãŸã¯ previous.json ãŒç©ºã®ãŸã‚ã€é€šçŸ¥ã›ãšä¿å­˜ã®ã¿è¡Œã„ã¾ã™ã€‚")
        save_current(current)
        return

    # å·®åˆ†ã‚’æ¤œå‡º
    new_list = detect_new_listings(current, previous)
    MAX_ITEMS = 5  # é€šçŸ¥ã™ã‚‹æœ€å¤§ä»¶æ•°

    if new_list:
        print(f"ğŸ”” {len(new_list)} ä»¶ã®æ–°ç€ç‰©ä»¶ã‚’æ¤œå‡ºï¼")
        message = f"ğŸ  æ–°ç€ç‰©ä»¶ä¸€è¦§ï¼ˆ{now.strftime('%Y/%m/%d %H:%M')} æ™‚ç‚¹ï¼‰\n\n"
        for item in new_list[:MAX_ITEMS]:
            message += f"{item['title']}\n{item['url']}\n\n"
        send_line_message(message.strip())
    else:
        print("ğŸ“­ æ–°ç€ãªã—ã€œ")

    # æœ€æ–°ã®ç‰©ä»¶ãƒªã‚¹ãƒˆã‚’ä¿å­˜
    save_current(current)

# ğŸ”¹ ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
if __name__ == "__main__":
    main()
