import os
import json
import requests
from bs4 import BeautifulSoup

# ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DATA_PATH = "previous.json"
NEW_ARRIVALS_PATH = "new_arrivals.json"

# URã®æ–°ç¯‰è³ƒè²¸ä½å®…ä¸€è¦§ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def fetch_ur_listings():
    url = "https://www.ur-net.go.jp/chintai/information/"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")
    base_url = "https://www.ur-net.go.jp"
    links = soup.select("a.item_link.fc_blue.item_cell")

    listings = []
    for link in links:
        text = link.get_text(strip=True)
        href = link.get("href")
        # ã€Œæ–°ç¯‰è³ƒè²¸ä½å®…ã€ã®æ–‡å­—åˆ—ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã ã‘ã‚’æŠ½å‡º
        if "æ–°ç¯‰è³ƒè²¸ä½å®…" in text:
            listings.append({"title": text, "url": base_url + href})
    return listings

# å‰å›ä¿å­˜ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
def load_previous():
    if os.path.exists(DATA_PATH):
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# JSONãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°
def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# æ–°ç€ã‚„æ›´æ–°æƒ…å ±ã‚’æ¤œå‡ºã™ã‚‹é–¢æ•°
def detect_new_listings(current, previous):
    previous_titles = {item["title"] for item in previous}
    new_items = [item for item in current if item["title"] not in previous_titles]

    new_arrivals = []
    updates = []

    for item in new_items:
        title = item["title"]
        if "æ–°è¦å…¥å±…è€…å‹Ÿé›†" in title:
            new_arrivals.append(item)
        elif any(kw in title for kw in ["æŠ½é¸å‹Ÿé›†", "å¿œå‹ŸçŠ¶æ³", "æŠ½é¸çµæœ"]):
            updates.append(item)

    return new_arrivals, updates

# LINEã‚°ãƒ«ãƒ¼ãƒ—ã«é€šçŸ¥ã‚’é€ã‚‹é–¢æ•°
def notify_line(message):
    token = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
    group_id = os.environ.get("LINE_GROUP_ID")

    if not token or not group_id:
        print("âŒ LINE_CHANNEL_ACCESS_TOKEN ã¾ãŸã¯ LINE_GROUP_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    data = {
        "to": group_id,
        "messages": [
            {
                "type": "text",
                "text": message
            }
        ]
    }

    try:
        res = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=data)
        print(f"ğŸ“¢ LINEã‚°ãƒ«ãƒ¼ãƒ—é€šçŸ¥é€ä¿¡ï¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {res.status_code}")
        print(res.text)
    except Exception as e:
        print(f"âš ï¸ LINEã‚°ãƒ«ãƒ¼ãƒ—é€šçŸ¥ã«å¤±æ•—: {e}")

# é€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ•´å½¢ã™ã‚‹é–¢æ•°
def format_message(header, items):
    lines = [header]
    for item in items:
        lines.append(f"ãƒ»{item['title']}\n{item['url']}")
    return "\n".join(lines)

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    current = fetch_ur_listings()           # ç¾åœ¨ã®ç‰©ä»¶ä¸€è¦§ã‚’å–å¾—
    previous = load_previous()              # å‰å›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    new_arrivals, updates = detect_new_listings(current, previous)  # å·®åˆ†ã‚’æ¤œå‡º

    print(f"ğŸŒ¿ new_arrivals: {len(new_arrivals)} ä»¶")
    print(f"ğŸŒ¿ updates: {len(updates)} ä»¶")

    if new_arrivals:
        save_json(NEW_ARRIVALS_PATH, new_arrivals)  # æ–°ç€ã‚’ä¿å­˜
        notify_line(format_message("ğŸ”” æ–°ç€ç‰©ä»¶ã®ãŠçŸ¥ã‚‰ã›", new_arrivals))  # LINEé€šçŸ¥
        save_json(DATA_PATH, current)  # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    elif updates:
        notify_line(format_message("ğŸ“„ æ›´æ–°æƒ…å ±ã®ãŠçŸ¥ã‚‰ã›", updates))  # LINEé€šçŸ¥
        save_json(DATA_PATH, current)  # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    else:
        print("ğŸ“­ æ–°ç€ã‚‚æ›´æ–°ã‚‚ãªã—ã€‚ã¾ãŸæ˜æ—¥ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã­ï¼")

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸã¨ãã ã‘main()ã‚’å‘¼ã¶
if __name__ == "__main__":
    main()
